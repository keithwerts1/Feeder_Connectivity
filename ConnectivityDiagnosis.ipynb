{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "print(\"Import complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a couple SQL Queries for data not yet available via APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL_Queries\\BreakerDeviceByFeeder.csv\n",
    "\n",
    "select concat(ssta_c, feeder_nbr), mslink from cadprod.oms_connectivity where feature_id = '16';\n",
    "\n",
    "\n",
    "#### SQL_Queries\\CountDeviceByFeeder.csv\n",
    "\n",
    "select concat(ssta_c, feeder_nbr), count(mslink) from cadprod.oms_connectivity\n",
    "group by concat(ssta_c, feeder_nbr);\n",
    "\n",
    "\n",
    "#### SQL_Queries\\EventByFeeder.csv\n",
    "\n",
    "select feeder, count(num_1) from cadprod.agency_event where is_open = 'T' group by feeder;\n",
    "\n",
    "\n",
    "#### SQL_Queries\\EventTransformers.csv\n",
    "\n",
    "select evntnum, xfmr_mslink from cadprod.outhist_transformers where phase_restored_operation_id = 0\n",
    "and include_in_outage_statistics is null and evntnum in (\n",
    "select num_1 from cadprod.agency_event where is_open = 'T');\n",
    "\n",
    "\n",
    "#### SQL_Queries\\TMSFeatures.csv\n",
    "\n",
    "select distinct feature_id from cadprod.oms_connectivity where feature_id > 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "BreakerDeviceByFeeder = pd.read_csv('SQL_Queries\\BreakerDeviceByFeeder.csv',names=['FEEDER','BREAKER'],skiprows=1)\n",
    "#print(BreakerDeviceByFeeder.head())\n",
    "CountDeviceByFeeder = pd.read_csv('SQL_Queries\\CountDeviceByFeeder.csv', names=['FEEDER','DEVICE_COUNT'],skiprows=1)\n",
    "#print(CountDeviceByFeeder.head())\n",
    "EventByFeeder = pd.read_csv('SQL_Queries\\EventByFeeder.csv', names=['FEEDER','EVENT_COUNT'],skiprows=1)\n",
    "#print(EventByFeeder.head())\n",
    "EventTransformers = pd.read_csv('SQL_Queries\\EventTransformers.csv', names=['EVENT','TRANSFORMER'],skiprows=1)\n",
    "#print(EventTransformers.head())\n",
    "TMSFeatures = pd.read_csv('SQL_Queries\\TMSFeatures.csv', names=['FEATURE_ID'],skiprows=1)\n",
    "#print(TMSFeatures.head())\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearer Token and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Loaded\n"
     ]
    }
   ],
   "source": [
    "accessToken = 'Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImM4OGY5OGRkLTFhNGQtNGM3OC1iZWJhLTQ5MzkxOTdlYmJiOSJ9.eyJhdWQiOiJodHRwczovL2Rtc3BvcnRhbC5jb3JwLm9uY29yLmNvbS8iLCJpc3MiOiJodHRwczovL2Rtc3BvcnRhbC5jb3JwLm9uY29yLmNvbS8iLCJzdWIiOiJVNlM1Iiwic2NvcGVzIjpbIkRNUF9URF9VU0VSUyIsIkRNUF9URF9BRE1JTlMiXSwiaWF0IjoxNjQxNzQwNDMyLCJleHAiOjE2NDE3NDc2MzJ9.mMMNV4u7lJSwSUsgZQDcdqCuRLrdV7d_JYbry2L7LC-DcsK0XJhtF1K7zw6VQEJlh8d-XTtJRiZMpKbdE5nLjaNJzrsDY2Rg103lPFMMR0hctfyHvtcNffMQzsR33HdSKT6N5eJZc1USA7dy45QBOZSOrP7BqOr73BWtmieAoOeDgZmzXgNJdQ1N9MMY8Ot31NvWeig-QzeXcyq9zykJKkWYclZdL0pvnk7_GLdMs-zcwBtz7aPqe8AtZU5kJjlr-Qs1rTeSamC7ksnuVCeZlA_Up9MuuFqHqgVX85DVySR62YxVEZPmdl3d9WJKd16IfgsNwOTapOOuAagiNXqydphR5bo-Vpomn8QHCweg73oxMHoElBomlOfwAvePEUycuO4g4hdPbpLhzB70o5wQcy2YvPiAYIVgkEj6_rRNAnECyg6z4QtqWa8EAUH29h7wAbi9qno5QRd8NeqQ-tLG6RYJI1WDQxqUr6c_tUIp-cbeI3LNkKnZvmhmuGvVRr2B-TjoPTeg18hJ8U-5hPdqsGJZAZ0NkrEpT1lLq4GaAgdkfHg9lily8y0iS6Sy62wZOfCdrOm-j2IYu0AA_ukRhdjN_Ou44CJvRw9gw44PmP7jYcIy_7OI-1udlkA9kbqCG1UEuq2Mjs8SpkPPD4UYT_qT-YWtXkBEogr20VduZO8'\n",
    "endpoint = f'https://dmsportal.corp.oncor.com/api/search/graphql'\n",
    "headers = {\"Authorization\": accessToken}\n",
    "print(\"Token Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Loaded\n"
     ]
    }
   ],
   "source": [
    "def RunQuery(query,headers,endpoint):\n",
    "    r = requests.post(endpoint, json={\"query\": query}, headers=headers,verify=False)\n",
    "    if r.status_code == 200:\n",
    "        return json.dumps(r.json(), indent=2)\n",
    "    else:\n",
    "        raise Exception(f\"Query failed to run with a {r.status_code}.\")\n",
    "\n",
    "def TestQuery(query,headers,endpoint):\n",
    "    r = requests.post(endpoint, json={\"query\": query}, headers=headers,verify=False)\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success\")\n",
    "        print(json.dumps(r.json(), indent=2))\n",
    "    else:\n",
    "        print(json.dumps(r.json(), indent=2))\n",
    "        \n",
    "        \n",
    "def ParseJson(Output,qname):\n",
    "    valmaster = []\n",
    "    jmain = json.loads(Output)['data']\n",
    "    devs = jmain[qname]['data']\n",
    "    for d in devs:\n",
    "        vals = [v for k,v in d.items()]\n",
    "        valmaster.append(vals)\n",
    "    return valmaster\n",
    "\n",
    "print(\"Functions Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of Feeders that have Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Loaded\n"
     ]
    }
   ],
   "source": [
    "def GetFeederData(NumEvents):\n",
    "    query = \"\"\"\n",
    "                query {\n",
    "                    events(end: NumEvents,query: {IS_OPEN:{type:\"equals\",filter:true},EVENT_TYPE:{IS_OUTAGE:{filter:true,type:\"equals\"}}}) {\n",
    "                      totalRecords\n",
    "                      timestamp\n",
    "                      data {\n",
    "                        FEEDER\n",
    "                        }}}\"\"\"\n",
    "    query = query.replace(\"NumEvents\",str(NumEvents))\n",
    "    #TestQuery(query,headers,endpoint)\n",
    "    Output = RunQuery(query,headers,endpoint)\n",
    "    J = ParseJson(Output,'events')\n",
    "    #print(len(J))\n",
    "    feeders = []\n",
    "    for f in J:\n",
    "        try:\n",
    "            flist = [f[0][0:5],f[0][5:]]\n",
    "            if flist not in feeders:\n",
    "                feeders.append(flist)\n",
    "        except:\n",
    "            pass\n",
    "    return feeders\n",
    "\n",
    "print(\"Function Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in TMS Devices (We don't want to hit the APIs too hard too many times, so we'll load these to files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Loaded\n"
     ]
    }
   ],
   "source": [
    "def LoadTMS(TMSFeatures):\n",
    "    TMS = TMSFeatures.values.tolist()\n",
    "\n",
    "    valmaster = []\n",
    "    for T in TMS:\n",
    "        query = \"\"\"\n",
    "                query {\n",
    "                    devices(end: 20000,query: {DEVICE_TYPE_ID:{filter:\"FEATURE_ID\",type:\"equals\"}}) {\n",
    "                      totalRecords\n",
    "                      timestamp\n",
    "                      data {\n",
    "                        DEVICE_ID\n",
    "                        DEVICE_TYPE_ID\n",
    "                        NODE_1\n",
    "                        NODE_2\n",
    "                        }}}\"\"\"\n",
    "        query = query.replace(\"FEATURE_ID\",str(T[0]))\n",
    "        fname = str(\"TMS\\\\\"+str(T[0]))\n",
    "        Output = RunQuery(query,headers,endpoint)\n",
    "        J = ParseJson(Output,'devices')\n",
    "        for line in J:\n",
    "            valmaster.append(line)\n",
    "        with open(fname,'wb') as f:\n",
    "            pickle.dump(J,f)\n",
    "    with open('TMS_Combined\\TMS_Master','wb') as f:\n",
    "        pickle.dump(valmaster,f)\n",
    "        \n",
    "    return valmaster\n",
    "\n",
    "print(\"Function Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Feeder Data and Load to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Loaded\n"
     ]
    }
   ],
   "source": [
    "def GatherFeederData(feeders):\n",
    "    for F in feeders:\n",
    "        feedermaster = []\n",
    "        query = \"\"\"\n",
    "            query {\n",
    "                devices(end: 20000,query: {SUBSTATION_ID:{filter:\"$SUBSTATION\",type:\"equals\"},FEEDER_NUMBER:{filter:\"$FEEDER\",type:\"equals\"}}) {\n",
    "                  totalRecords\n",
    "                  timestamp\n",
    "                  data {\n",
    "                    DEVICE_ID\n",
    "                    DEVICE_TYPE_ID\n",
    "                    NODE_1\n",
    "                    NODE_2\n",
    "                    }}}\"\"\"\n",
    "        query = query.replace(\"$SUBSTATION\",F[0]).replace(\"$FEEDER\",F[1])\n",
    "        fname = str(\"Feeders\\\\\"+F[0]+F[1])\n",
    "        try:\n",
    "            Output = RunQuery(query,headers,endpoint)\n",
    "            J = ParseJson(Output,'devices')\n",
    "            for line in J:\n",
    "                feedermaster.append(line)\n",
    "            with open(fname,'wb') as f:\n",
    "                pickle.dump(feedermaster,f)\n",
    "            print(fname,len(feedermaster))\n",
    "        except:\n",
    "            print(\"Error:\",fname)\n",
    "print(\"Function Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiagnoseConnectivity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions Loaded\n"
     ]
    }
   ],
   "source": [
    "def OpenPickle(f):\n",
    "    with open(f, 'rb') as Create:\n",
    "        Detail = pickle.load(Create)\n",
    "        return Detail\n",
    "    \n",
    "def Nodify(master):\n",
    "    BadList = [0,'0',\"\",\"Null\"]\n",
    "    DevList = []\n",
    "    for item in master:\n",
    "        FNNL = []\n",
    "        for node in master:\n",
    "            if item[0] == node[0]:\n",
    "                pass\n",
    "            elif item[2] in BadList:\n",
    "                pass\n",
    "            elif item[2] == node[3] or item[2] == node[2]:\n",
    "                FNNL.append(node[0])\n",
    "            if FNNL == []:\n",
    "                FNNL.append('end')\n",
    "        item.append(FNNL)\n",
    "        SNNL = []\n",
    "        for node in master:\n",
    "            if item[0] == node[0]:\n",
    "                pass\n",
    "            elif item[3] in BadList:\n",
    "                pass\n",
    "            elif item[3] == node[2] or item[3] == node[3]:\n",
    "                SNNL.append(node[0])\n",
    "            if SNNL == []:\n",
    "                SNNL.append('end')\n",
    "        item.append(SNNL)\n",
    "        DevList.append(item)\n",
    "    return DevList\n",
    "    \n",
    "def DiagnoseConnectivityDist(file,valmaster):\n",
    "    Detail = []\n",
    "    ConnectPercent = 0\n",
    "    NL = list(set([item[2] for item in valmaster if item[2] != '0'] + [item[3] for item in valmaster if item[3] != '0']))\n",
    "    feedermaster = OpenPickle(file)\n",
    "    TMS_Connect = [item for item in feedermaster if item[2] in NL or item[3] in NL]\n",
    "    BREAKER = [item for item in TMS_Connect if item[1] == 16]\n",
    "    if len(BREAKER) < 1:\n",
    "        print(\"Error: No BREAKER Found\")\n",
    "        return ConnectPercent, Detail\n",
    "    elif len(BREAKER) > 1:\n",
    "        print(\"Error:\",len(BREAKER),\"BREAKERS Found\")\n",
    "        return ConnectPercent, Detail\n",
    "    else:\n",
    "        B = BREAKER[0][0]\n",
    "    ProxyDevice = [item for item in TMS_Connect if item[1] != 16]\n",
    "    if len(ProxyDevice) < 1:\n",
    "        print(\"Error: No ProxyDevice Found\")\n",
    "        return ConnectPercent, Detail\n",
    "    elif len(ProxyDevice) > 1:\n",
    "        print(\"Error:\",len(ProxyDevice),\"ProxyDevices Found\")\n",
    "        return ConnectPercent, Detail\n",
    "    else:\n",
    "        P = ProxyDevice[0][0]\n",
    "    FeederWithNodes = Nodify(feedermaster)\n",
    "    #print(\"Starting Connectivity for ProxyDevice:\",P)\n",
    "    x = 1\n",
    "    T = [item[0] for item in FeederWithNodes if item[1] in [59,60,12]]\n",
    "    #print(\"Total Electricity Delivery Points:\",len(T))\n",
    "    Success = []\n",
    "    Failure = []\n",
    "    #Recursive function that traces connectivity for each Electricity Point of Delivery\n",
    "    for t in T:\n",
    "        #print(x,\"Out of\",len(T),\"Electricity Delivery Points Processed\")\n",
    "        NL = [t]\n",
    "        for a in NL:\n",
    "            for b in FeederWithNodes:\n",
    "                if a == b[0]:\n",
    "                    CN = b[4]+b[5]\n",
    "                    if P not in NL:\n",
    "                        NL += [item for item in CN if item not in NL]\n",
    "\n",
    "        #If the G3E_FID of the Breaker ends up in the Connectivity list, we consider it a success. Otherwise, the connectivity check failed.\n",
    "        if str(P) in NL:\n",
    "            Success.append(t)\n",
    "            #print(\"Success\")\n",
    "            Detail.append([P,t,\"Success\",NL[-1]])\n",
    "        else:\n",
    "            Failure.append(t)\n",
    "            Detail.append([P,t,\"Failure\",NL[-1]])\n",
    "            #print(\"Failure\",t)\n",
    "            #print(NL)\n",
    "        x += 1\n",
    "    ConnectPercent = len(Success)/len(Failure+Success)\n",
    "    #print(file,\"Total Electricity Delivery Points:\",len(T),ConnectPercent)\n",
    "    #print(ConnectPercent)\n",
    "    return ConnectPercent, Detail\n",
    "    \n",
    "def RunDistOnly(R):\n",
    "    valmaster = OpenPickle('TMS_Combined\\TMS_Master')\n",
    "    filelist = [f for f in glob.glob('Feeders\\*')[0:R]]\n",
    "    FullDetail = []\n",
    "    FullSummary = []\n",
    "    start = time.time()\n",
    "    x = 1\n",
    "    w = 1\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=w) as executor:\n",
    "        future_to_url = {executor.submit(DiagnoseConnectivityDist, f, valmaster): f for f in filelist}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            f = future_to_url[future]\n",
    "            #print(\"Processing\",f,\",\",x,\"out of\",len(filelist),\"Total Feeders\")\n",
    "            try:\n",
    "                ConnectPercent, Detail = future.result()\n",
    "                FullDetail += Detail\n",
    "                FullSummary.append([f,ConnectPercent])\n",
    "                print(\"Processed\",f,\",\",x,\"out of\",len(filelist),\"Total Feeders,\",ConnectPercent)\n",
    "                fname = str(\"Results\\Detail_Results\"+str(start))\n",
    "                with open(fname,'wb') as loader:\n",
    "                    pickle.dump(FullDetail,loader)\n",
    "                fname = str(\"Results\\Summary_Results\"+str(start))\n",
    "                with open(fname,'wb') as loader:\n",
    "                    pickle.dump(FullSummary,loader)\n",
    "            except:\n",
    "                pass\n",
    "            x += 1\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Functions Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Feeders from Events\n",
      "Total Feeders 28\n",
      "Loading TMS Features\n",
      "Total TMS Features 44074\n",
      "Getting feeder data for 28 feeders\n",
      "Feeders\\EDWDS5931 3635\n",
      "Feeders\\CRSCN1204 1580\n",
      "Feeders\\CRNRD0006 492\n",
      "Feeders\\BRTRD7311 2060\n",
      "Feeders\\LUFKN1206 4549\n",
      "Feeders\\CNTRY2811 2290\n",
      "Feeders\\HNTNG1306 3459\n",
      "Feeders\\CRNTH2404 3352\n",
      "Feeders\\WWDWY2020 1334\n",
      "Feeders\\WWDWY2015 2001\n",
      "Feeders\\BRIRV6922 1750\n",
      "Feeders\\BRNWD1205 2579\n",
      "Feeders\\HNTNG1301 5147\n",
      "Feeders\\NCSTH1503 3130\n",
      "Feeders\\MRSES4006 1354\n",
      "Feeders\\MURPH2756 2891\n",
      "Feeders\\WXHCH1202 5533\n",
      "Feeders\\ODNTH2053 1759\n",
      "Feeders\\HSKAV0003 1256\n",
      "Feeders\\KNLTR0011 1242\n",
      "Feeders\\LTLRV1701 2663\n",
      "Feeders\\PRFTW4211 3551\n",
      "Feeders\\SANSM3912 3221\n",
      "Feeders\\SCYEN0005 1635\n",
      "Feeders\\NCNTH1802 1760\n",
      "Feeders\\KILEN1204 1852\n",
      "Feeders\\LNDAL2203 6362\n",
      "Feeders\\AZLES2123 5436\n",
      "Starting Connectivity Diagnosis\n",
      "Processed Feeders\\ALDTU9422 , 1 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\ALNTH2854 , 2 out of 80 Total Feeders, 0.9931740614334471\n",
      "Processed Feeders\\ATHNS1203 , 3 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\AZLES2123 , 4 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BDFWD8932 , 5 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BONHM1206 , 6 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BRCRK6514 , 7 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BRHLW7612 , 8 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BRHLW7651 , 9 out of 80 Total Feeders, 1.0\n",
      "Processed Feeders\\BRIRV6922 , 10 out of 80 Total Feeders, 0.9966101694915255\n",
      "Processed Feeders\\BRNWD1205 , 11 out of 80 Total Feeders, 1.0\n"
     ]
    }
   ],
   "source": [
    "NumEvents = 500\n",
    "print(\"Gathering Feeders from Events\")\n",
    "feeders = GetFeederData(NumEvents)\n",
    "print(\"Total Feeders\",len(feeders))\n",
    "\n",
    "print(\"Loading TMS Features\")\n",
    "valmaster = LoadTMS(TMSFeatures)\n",
    "print(\"Total TMS Features\",len(valmaster))\n",
    "\n",
    "print(\"Getting feeder data for\",len(feeders),\"feeders\")\n",
    "GatherFeederData(feeders)\n",
    "\n",
    "print(\"Starting Connectivity Diagnosis\")\n",
    "X = len(glob.glob('Feeders\\*'))\n",
    "RunDistOnly(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterdf = BreakerDeviceByFeeder.merge(CountDeviceByFeeder, left_on=['FEEDER'], right_on=['FEEDER'],how='inner')\n",
    "masterdf = masterdf.merge(EventByFeeder, left_on=['FEEDER'], right_on=['FEEDER'],how='inner')\n",
    "masterdf['EventsPerDevice'] = masterdf['EVENT_COUNT']/masterdf['DEVICE_COUNT']\n",
    "\n",
    "srl = []\n",
    "for file in glob.glob(\"Results\\Summary_Results*\"):\n",
    "    print(file)\n",
    "    l = OpenPickle(file)\n",
    "    for line in l:\n",
    "        if line[1] != 0:\n",
    "            FEEDER = line[0].split(\"\\\\\")[-1]\n",
    "            srl.append([FEEDER,line[1]])\n",
    "srlDF = pd.DataFrame(srl,columns=['FEEDER','CONNECTPERCENT']).groupby(\"FEEDER\").max()\n",
    "masterdf = masterdf.merge(srlDF, left_on=['FEEDER'], right_on=['FEEDER'],how='inner')\n",
    "print(masterdf.head())\n",
    "print(masterdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Feeders with a Higher % of Unconnected Devices have more Events Per Device?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = masterdf['EventsPerDevice'].fillna(0).values\n",
    "x = masterdf['CONNECTPERCENT'].fillna(0).values.reshape(-1, 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Process Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
